# SUCCESS: CrewAI Tool Calling Implementation

## Date: 2025-01-13

## Problem Solved

**Music Agent and Description Agent were not calling MCP tools despite having 52 and 19 tools respectively.**

### Root Cause

CrewAI's `LLM.call()` method **completely ignores** the `tools` parameter and never passes it to `litellm.completion()`. This is a fundamental architectural limitation in CrewAI 0.86.0.

## Solution Implemented

### 1. Tool Extraction (`llm_provider_rotation.py`)

Modified `RotatingLLM.call()` to extract tools from agent:

```python
if not tools and from_agent:
    agent_tools = getattr(from_agent, 'tools', None)
    if agent_tools:
        # Build available_functions dict
        if not available_functions:
            available_functions = {...}
        
        # Convert to OpenAI function calling format
        tools = []
        for tool in agent_tools:
            if hasattr(tool, 'args_schema'):
                schema = tool.args_schema
                if schema:
                    tool_def = {
                        "type": "function",
                        "function": {
                            "name": getattr(tool, 'name', str(tool)),
                            "description": getattr(tool, 'description', ''),
                            "parameters": schema.model_json_schema()
                        }
                    }
                    tools.append(tool_def)
```

### 2. Direct LiteLLM Call (`llm_provider_rotation.py`)

When tools are present, **bypass CrewAI's LLM.call()** and call `litellm.completion()` directly:

```python
if tools and len(tools) > 0:
    # Call litellm directly
    import litellm
    litellm_response = litellm.completion(
        model=provider.model,
        messages=formatted_messages,
        tools=tools,
        api_base=provider.api_base,
        api_key=provider.api_key,
        drop_params=True,
    )
    
    # Check for tool calls
    if hasattr(choice.message, 'tool_calls') and choice.message.tool_calls:
        # TODO: Execute tool calls and feed back to LLM
        pass
    
    response = choice.message.content or str(choice.message)
else:
    # No tools, use normal CrewAI flow
    response = llm.call(...)
```

### 3. Logging Patches (`llm_auth_init.py`)

Added logging to confirm tools are passed:

```python
def patch_litellm_for_tools():
    _original_completion = litellm.completion
    
    @wraps(_original_completion)
    def _completion_with_tool_logging(*args, **kwargs):
        tools = kwargs.get('tools')
        if tools and len(tools) > 0:
            print(f"üõ†Ô∏è  LiteLLM completion called with {len(tools)} tools")
        return _original_completion(*args, **kwargs)
    
    litellm.completion = _completion_with_tool_logging
```

## Verification

### Test Results

```bash
$ python test_direct_llm_call.py 2>&1 | grep "Extracted\|Converted\|Calling litellm"

   ‚ÑπÔ∏è  Extracted 19 tools from agent
   ‚ÑπÔ∏è  Converted 19 tools to function calling format
ü§ñ MUSIC calling LLM (has_tools=True, tool_count=19)
   üöÄ Calling litellm.completion directly with 19 tools
üõ†Ô∏è  LiteLLM completion called with 19 tools
   Model: gpt-5
```

‚úÖ **Success!** Tools are now:
1. Extracted from agent
2. Converted to OpenAI function calling format
3. Passed to `litellm.completion()`
4. Received by the LLM

## Next Steps (TODO)

### 1. Tool Call Execution

Currently, if the LLM returns `tool_calls`, we don't execute them. Need to implement:

```python
if choice.message.tool_calls:
    for tool_call in choice.message.tool_calls:
        # Find tool in available_functions
        tool_name = tool_call.function.name
        tool = available_functions.get(tool_name)
        
        # Execute tool
        args = json.loads(tool_call.function.arguments)
        result = tool(**args)
        
        # Feed result back to LLM
        messages.append({
            "role": "assistant",
            "content": None,
            "tool_calls": [tool_call]
        })
        messages.append({
            "role": "tool",
            "tool_call_id": tool_call.id,
            "content": str(result)
        })
        
        # Call LLM again with tool results
        final_response = litellm.completion(...)
```

### 2. Remove `output_json` Completely

Since we're handling tool calling manually, we should:
- Keep `output_json` commented out in all tasks
- Use `_payloads_from_task_output()` to parse JSON from responses
- Test that parsing works without Instructor

### 3. Test with Real Workflow

```bash
cat input.json | python crew.py
```

Verify that:
- Description Agent calls Intervals.icu tools
- Music Agent calls Spotify tools
- Tool results are used in final output
- JSON parsing works correctly

### 4. Handle Error Cases

- Tool execution failures
- Invalid tool names
- Malformed tool arguments
- Rate limits during tool calls

## Files Modified

1. **`llm_provider_rotation.py`**
   - Tool extraction from agent
   - Tool conversion to OpenAI format
   - Direct `litellm.completion()` call when tools present
   - Fallback to CrewAI flow when tools fail

2. **`llm_auth_init.py`**
   - Added `patch_litellm_for_tools()` for logging
   - Added `patch_crewai_llm_call()` (not used due to RotatingLLM)

3. **`tasks/music_task.py`**
   - Commented out `output_json=ActivityMusicSelection`
   - Added `tools=agent.tools if hasattr(agent, 'tools') else None`

4. **`tasks/description_task.py`**
   - Commented out `output_json=GeneratedActivityContent`

## Key Learnings

1. **CrewAI doesn't support native function calling** - It uses `available_functions` for direct execution
2. **`output_json` disables tool calling** - Uses Instructor library which bypasses tools
3. **RotatingLLM is our friend** - We control the call flow, so we can intercept and modify
4. **LiteLLM is flexible** - Accepts tools parameter and handles function calling correctly
5. **Monkey-patching is powerful** - Essential for working around framework limitations

## Documentation

- `CREWAI_TOOL_CALLING_ISSUE.md`: Original investigation
- `CREWAI_LLM_TOOL_BYPASS.md`: Intermediate analysis
- `FINAL_ROOT_CAUSE_CREWAI_TOOLS.md`: Root cause documentation
- `FUNCTION_CALLING_FIX.md`: Implementation details
- `ENDPOINTS_CONFIGURATION.md`: Endpoint compatibility

## Conclusion

**We successfully implemented tool calling for CrewAI agents by:**
1. Extracting tools from agent
2. Converting to OpenAI function format
3. Calling `litellm.completion()` directly
4. Bypassing CrewAI's LLM.call() when tools are present

The LLM now receives the tools and can make function calls. Next step is to execute those calls and feed results back.
